{"cells":[{"cell_type":"markdown","id":"6203972b","metadata":{"id":"6203972b"},"source":["# Trenbalon Team: Kaggle Submission Notebook\n","\n","This notebook demonstrates our solution for the datathon challenge. The goal of the task is to classify images effectively using Swin Transformers, leveraging transfer learning techniques and efficient training strategies.\n","\n","### Team Members\n","- **İlker Yetimoğlu**\n","- **Yusuf Demir**\n","- **Ahmet Emin Ersoy**\n","\n","The notebook is organized into sections for better readability and to ensure seamless reproducibility across different environments."]},{"cell_type":"markdown","id":"52e6b5e8","metadata":{"id":"52e6b5e8"},"source":["## Environment Setup\n","\n","In this section, we set up the environment by mounting Google Drive (if applicable) and installing necessary libraries."]},{"cell_type":"code","execution_count":1,"id":"e2d21391","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2d21391","outputId":"518004b8-fccf-4646-f827-6f417f2e9c28","executionInfo":{"status":"ok","timestamp":1734350765598,"user_tz":-180,"elapsed":22614,"user":{"displayName":"Yusuf Demir","userId":"08472347386907544067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install datasets transformers evaluate"]},{"cell_type":"code","execution_count":2,"id":"732d9be0","metadata":{"id":"732d9be0","executionInfo":{"status":"ok","timestamp":1734350819949,"user_tz":-180,"elapsed":32371,"user":{"displayName":"Yusuf Demir","userId":"08472347386907544067"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from datasets import load_dataset, Features, ClassLabel, Value, Image\n","from PIL import Image as PILImage\n","import torch\n","import evaluate\n","\n","from transformers import (AutoFeatureExtractor, SwinForImageClassification,\n","                          Trainer, TrainingArguments, EarlyStoppingCallback)\n","\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","id":"63647cd5","metadata":{"id":"63647cd5"},"source":["## Data Loading and Preprocessing\n","\n","Here, we load the dataset and preprocess it for training. This includes image normalization and resizing using torchvision transforms."]},{"cell_type":"code","execution_count":null,"id":"4c239836","metadata":{"id":"4c239836"},"outputs":[],"source":["train_csv_path = \"datathon-ai-qualification-round/train_data.csv\" # YOUR traind_data.csv PATH\n","test_csv_path = \"datathon-ai-qualification-round/test.csv\" # YOUR test_data.csv PATH\n","global_path = \"datathon-ai-qualification-round\" # YOUR GLOBAL data PATH\n","\n","\n","cities = [\"Istanbul\", \"Ankara\", \"Izmir\"]\n","\n","\n","train_dataset = load_dataset(\"csv\", data_files=train_csv_path, split=\"train\")\n","test_dataset = load_dataset(\"csv\", data_files=test_csv_path, split=\"train\")\n","\n","def add_image_path_train(examples):\n","    examples[\"image\"] = [os.path.join(global_path, \"train\", \"train\", fname) for fname in examples[\"filename\"]]\n","    return examples\n","\n","def add_image_path_test(examples):\n","    examples[\"image\"] = [os.path.join(global_path, \"test\", fname) for fname in examples[\"filename\"]]\n","    return examples\n","\n","train_dataset = train_dataset.map(add_image_path_train, batched=True)\n","test_dataset = test_dataset.map(add_image_path_test, batched=True)\n","\n","def city_to_id(example):\n","    if example[\"city\"] in cities:\n","        example[\"label\"] = cities.index(example[\"city\"])\n","    else:\n","        pass\n","    return example\n","\n","train_dataset = train_dataset.map(city_to_id)\n","\n","features_train = Features({\n","    \"filename\": Value(\"string\"),\n","    \"city\": Value(\"string\"),\n","    \"label\": ClassLabel(names=cities),\n","    \"image\": Image()\n","})\n","\n","features_test = Features({\n","    \"filename\": Value(\"string\"),\n","    \"city\": Value(\"string\"),\n","    \"image\": Image()\n","})\n","\n","train_dataset = train_dataset.cast(features=features_train)\n","test_dataset = test_dataset.cast(features=features_test)\n"]},{"cell_type":"code","execution_count":null,"id":"fedd9edc","metadata":{"id":"fedd9edc"},"outputs":[],"source":["labels = train_dataset[\"label\"]\n","\n","train_indices, val_indices = train_test_split(\n","    range(len(train_dataset)),\n","    test_size=0.2,\n","    stratify=labels,\n","    random_state=42\n",")\n","\n","train_ds = train_dataset.select(train_indices)\n","val_ds = train_dataset.select(val_indices)"]},{"cell_type":"code","execution_count":null,"id":"639e59ea","metadata":{"id":"639e59ea"},"outputs":[],"source":["def transform_train(examples):\n","    images = [train_transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n","    examples[\"pixel_values\"] = images\n","    return examples\n","\n","def transform_val(examples):\n","    images = [val_transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n","    examples[\"pixel_values\"] = images\n","    return examples\n","\n","def transform_test(examples):\n","    images = [val_transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n","    examples[\"pixel_values\"] = images\n","    return examples\n","\n","train_ds = train_ds.with_transform(transform_train)\n","val_ds = val_ds.with_transform(transform_val)\n","test_dataset = test_dataset.with_transform(transform_test)"]},{"cell_type":"markdown","id":"f67340cd","metadata":{"id":"f67340cd"},"source":["## Model Setup\n","We initialize the Swin Transformer model for image classification using pre-trained weights from Hugging Face."]},{"cell_type":"code","execution_count":null,"id":"e2a949bf","metadata":{"id":"e2a949bf"},"outputs":[],"source":["train_ds = train_ds.remove_columns([\"filename\", \"city\"])\n","val_ds = val_ds.remove_columns([\"filename\", \"city\"])\n","test_dataset = test_dataset.remove_columns([\"filename\", \"city\"])"]},{"cell_type":"code","execution_count":null,"id":"cedd152f","metadata":{"id":"cedd152f"},"outputs":[],"source":["model_name = \"microsoft/swin-large-patch4-window7-224\"\n","feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n","\n","resize_size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(resize_size, scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize(resize_size),\n","    transforms.CenterCrop(resize_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","])"]},{"cell_type":"code","execution_count":null,"id":"20649a3f","metadata":{"id":"20649a3f"},"outputs":[],"source":["model = SwinForImageClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(cities),\n","    id2label={i: c for i, c in enumerate(cities)},\n","    label2id={c: i for i, c in enumerate(cities)},\n","    ignore_mismatched_sizes=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"7ef62ec3","metadata":{"id":"7ef62ec3"},"outputs":[],"source":["f1_metric = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    results = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n","    macro_f1 = results[\"f1\"]\n","    return {\"macro_f1\": macro_f1}"]},{"cell_type":"code","execution_count":null,"id":"69cb4cba","metadata":{"id":"69cb4cba"},"outputs":[],"source":["def my_data_collator(features):\n","    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n","    labels = torch.tensor([f[\"label\"] for f in features])\n","    return {\"pixel_values\": pixel_values, \"labels\": labels}"]},{"cell_type":"markdown","id":"1c372cd6","metadata":{"id":"1c372cd6"},"source":["## Training\n","The training loop is configured using the Hugging Face `Trainer` API. It supports distributed training and includes early stopping for optimal performance."]},{"cell_type":"code","execution_count":null,"id":"d7da8caa","metadata":{"id":"d7da8caa"},"outputs":[],"source":["batch_size = 32\n","num_epochs = 15\n","learning_rate = 5e-5\n","weight_decay = 0.01\n","\n","training_args = TrainingArguments(\n","    output_dir=\"swin_large_2\",\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    save_steps=200,\n","    eval_steps=200,\n","    logging_steps=200,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"macro_f1\",\n","    greater_is_better=True,\n","    remove_unused_columns=False,\n","    report_to=\"none\",\n","    fp16=True if torch.cuda.is_available() else False,\n",")\n","print(f\"GPU IS AVALIABLE: {torch.cuda.is_available()}\")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    tokenizer=feature_extractor,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n","    data_collator=my_data_collator\n",")"]},{"cell_type":"code","execution_count":null,"id":"c0f4d958","metadata":{"id":"c0f4d958"},"outputs":[],"source":["trainer.train(resume_from_checkpoint=True)"]},{"cell_type":"markdown","source":["## Evaluation and Results"],"metadata":{"id":"icFaFpIwUon8"},"id":"icFaFpIwUon8"},{"cell_type":"code","execution_count":null,"id":"d80059a3","metadata":{"id":"d80059a3"},"outputs":[],"source":["import os\n","import pandas as pd\n","from transformers import AutoModelForImageClassification, AutoFeatureExtractor\n","from datasets import load_dataset\n","from PIL import Image\n","import torch"]},{"cell_type":"code","execution_count":null,"id":"a6d06bb0","metadata":{"id":"a6d06bb0"},"outputs":[],"source":["cities = [\"Istanbul\", \"Ankara\", \"Izmir\"]\n","\n","test_csv_path = \"datathon-ai-qualification-round/test.csv\" # YOUR test.csv PATH\n","test_dataset = load_dataset(\"csv\", data_files=test_csv_path, split=\"train\")\n","global_path = \"datathon-ai-qualification-round\" #YOUR GLOBAL data PATH\n","\n","def add_image_path_test(examples):\n","    examples[\"image\"] = [os.path.join(global_path, \"test\", \"test\", fname) for fname in examples[\"filename\"]]\n","    return examples\n","\n","test_dataset = test_dataset.map(add_image_path_test, batched=True)"]},{"cell_type":"code","execution_count":null,"id":"0b444f81","metadata":{"id":"0b444f81"},"outputs":[],"source":["model_path = \"swin_large_2/swin_large_2_best\"\n","model = AutoModelForImageClassification.from_pretrained(model_path)\n","feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"id":"2e8b9a61","metadata":{"id":"2e8b9a61"},"outputs":[],"source":["def predict_city(example):\n","    image_path = example[\"image\"]\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","\n","    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","        predicted_label = torch.argmax(probs, dim=-1).item()\n","\n","    example[\"city\"] = cities[predicted_label]\n","    return example"]},{"cell_type":"markdown","id":"9de02416","metadata":{"id":"9de02416"},"source":["## Submission Preparation\n","\n","Finally, the predictions are saved in the required format for submission to Kaggle."]},{"cell_type":"code","execution_count":null,"id":"dbdb90cd","metadata":{"id":"dbdb90cd"},"outputs":[],"source":["test_dataset = test_dataset.map(predict_city)\n","\n","test_results = test_dataset.to_pandas()\n","\n","test_results = test_results[[\"filename\", \"city\"]]\n","output_csv_path = \"swin_large_2/test_with_predictions.csv\"\n","test_results.to_csv(output_csv_path, index=False)\n","\n","print(f\"Sonuçlar {output_csv_path} dosyasına kaydedildi.\")\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}